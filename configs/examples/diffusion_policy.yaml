alg: DiffusionPolicy
alg_kwargs:
  # Configure offline steps. These aren't needed, but good to set.
  offline_steps: -1
  random_steps: 0
  scheduler: ["import", "diffusers.schedulers.scheduling_ddim", "DDIMScheduler"]
  scheduler_kwargs:
    num_train_timesteps: 100
    beta_start: 0.0001
    beta_send: 0.02
    beta_schedule: squaredcos_cap_v2
    clip_sample: True
    set_alpha_to_one: True
    steps_offset: 0
    prediction_type: epsilon
  num_inference_steps: 20
  horizon: 16

optim: AdamW
optim_kwargs:
  lr: 0.0001
  betas:
    - 0.95
    - 0.999
  eps: 1.0e-08
  weight_decay: 1.0e-06

network: ActorPolicy
network_kwargs:
  encoder_class: MultiEncoder
  encoder_kwargs:
    wrist_image_class: RobomimicEncoder
    wrist_image_kwargs:
      backbone: 18
      feature_dim: 64
      use_group_norm: True
      num_kp: 64
    agent_image_class: RobomimicEncoder
    agent_image_kwargs:
      backbone: 18
      feature_dim: 64
      use_group_norm: True
      num_kp: 64
    state_class: ["import", "torch.nn", "Identity"] 
  actor_class: Conditional1DUnet
  actor_kwargs:
    diffusion_step_embed_dim: 128
    down_dims: [256, 512, 1024] # This is [512, 1024, 2048]
    kernel_size: 5
    n_groups: 8

# Example config using Robomimic.
eval_env: RobomimicEnv
eval_env_kwargs:
  path: path/to/robomimic/dataset
  horizon: 400

dataset: RobomimicDataset
dataset_kwargs:
  path: path/to/robomimic/dataset
  sample_fn: sample
  sample_kwargs:
    batch_size: 64
    seq: 16
    pad: 8
    seq_keys: ["action"]
validation_dataset_kwargs:
  train: False

schedule:
  actor: cosine_with_linear_warmup
schedule_kwargs:
  actor:
    warmup_steps: 2000
    total_steps: 500000

processor: RandomCrop

trainer_kwargs:
  total_steps: 500000
  log_freq: 250 # How often to log values
  profile_freq: 250
  eval_freq: 10000 # How often to run evals
  eval_fn: null
  loss_metric: loss # The validation metric that determines when to save the "best_checkpoint"
  max_validation_steps: 20 # Will run forever otherwise due to continuous replay buffer iter.
  train_dataloader_kwargs:
    num_workers: 4 # Number of dataloader workers.
    batch_size: null
    collate_fn: null